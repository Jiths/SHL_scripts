{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sparse Hebbian Learning : reproducing SparseNet\n",
    "\n",
    "In this notebook, we test the convergence of SparseNet as a function of different learning parameters. This shows the relative robusteness of this method according to the coding parameters, but also the importance of homeostasis to obtain an efficient set of filters.\n",
    "\n",
    "See also :\n",
    "* http://blog.invibe.net/posts/2015-05-05-reproducing-olshausens-classical-sparsenet.html for a description of how SparseNet is implemented in the scikit-learn package\n",
    "* http://blog.invibe.net/posts/2015-05-06-reproducing-olshausens-classical-sparsenet-part-2.html for a descrtiption of how we managed to implement the homeostasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproducible research : Python implementation of SparseHebbianLearning\n",
      "======================================================================\n",
      "\n",
      "\n",
      ".. image:: assc.png\n",
      "   :scale: 100%\n",
      "   :alt: Set of RFs after aSSC learning.\n",
      " \n",
      "\n",
      "Object\n",
      "------\n",
      "\n",
      "* This is a collection of python scripts to test learning strategies to efficiently code natural image patches.  This is here restricted  to the framework of the SparseNet algorithm from Bruno Olshausen (http://redwood.berkeley.edu/bruno/sparsenet/).\n",
      "\n",
      "* this has been published as Perrinet, Neural Computation (2010) (see  http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl )::\n",
      "\n",
      "   @article{Perrinet10shl,\n",
      "        Author = {Perrinet, Laurent U.},\n",
      "        Title = {Role of homeostasis in learning sparse representations},\n",
      "        Year = {2010}\n",
      "        Url = {http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl},\n",
      "        Doi = {10.1162/neco.2010.05-08-795},\n",
      "        Journal = {Neural Computation},\n",
      "        Volume = {22},\n",
      "        Number = {7},\n",
      "        Keywords = {Neural population coding, Unsupervised learning, Statistics of natural images, Simple cell receptive fields, Sparse Hebbian Learning, Adaptive Matching Pursuit, Cooperative Homeostasis, Competition-Optimized Matching Pursuit},\n",
      "        Month = {July},\n",
      "        }\n",
      "\n",
      "* all comments and bug corrections should be submitted to Laurent Perrinet at Laurent.Perrinet@gmail.com\n",
      "* find out updates on http://invibe.net/LaurentPerrinet/SparseHebbianLearning\n",
      "\n",
      "\n",
      "Installation\n",
      "-------------\n",
      "\n",
      "* Be sure to have dependencies installed::\n",
      "\n",
      "   pip3 install -U git+https://github.com/NeuralEnsemble/NeuroTools.git\n",
      "   pip3 install -U git+https://github.com/bicv/SLIP.git\n",
      "   pip3 install -U git+https://github.com/laurentperrinet/scikit-learn.git@sparsenet\n",
      "\n",
      "* Then, download the code @ https://github.com/laurentperrinet/shl_scripts/archive/master.zip. You may also grab it directly using the command-line::\n",
      "\n",
      "   wget https://github.com/laurentperrinet/shl_scripts/archive/master.zip\n",
      "   unzip master.zip -d shl_scripts\n",
      "   cd shl_scripts/\n",
      "   ipython setup.py clean build install\n",
      "   jupyter notebook\n",
      "\n",
      "* developpers may use all the power of git with::\n",
      "\n",
      "   git clone https://github.com/laurentperrinet/SHL_scripts.git\n",
      "\n",
      "Licence\n",
      "--------\n",
      "\n",
      "This piece of code is distributed under the terms of the GNU General Public License (GPL), check http://www.gnu.org/copyleft/gpl.html if you have not red the term of the license yet.\n",
      "\n",
      "Contents\n",
      "--------\n",
      "\n",
      "* ``README.rst`` : this file\n",
      "* ``src/shl_scripts.py`` : the class file\n",
      "* ``*.ipynb`` : the individual experiments as notebooks\n",
      "* ``database`` : the image files.\n",
      "\n",
      "Changelog\n",
      "---------\n",
      "\n",
      "* 2.1 -2015-10-20:\n",
      " * finalizing the code to reproduce the sparsenet algorithm\n",
      "\n",
      "* 2.0 - 2015-05-07:\n",
      " * transform to a class to just do the Sparse Hebbian Learning (high-level) experiments (getting data from an image folder, learning, coding, analyszing)\n",
      " * use sklearn to do all the hard low-level work, in particular ``sklearn.decomposition.SparseCoder`` see http://scikit-learn.org/stable/auto_examples/decomposition/plot_image_denoising.html and http://www.cs.utexas.edu/~leif/pubs/20130626-scipy-johnson.pdf\n",
      " * The dictionary learning is tested in http://blog.invibe.net/posts/2015-05-05-reproducing-olshausens-classical-sparsenet.html and the corresponding PR is tested in http://blog.invibe.net/posts/2015-05-06-reproducing-olshausens-classical-sparsenet-part-2.html\n",
      "\n",
      "* 1.1 - 2014-06-18:\n",
      " * documentation\n",
      " * dropped Matlab support\n",
      "\n",
      "* 1.0 - 2011-10-27 : initial release\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run setup.py --long-description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shl_scripts import SHL\n",
    "DEBUG_DOWNSCALE, verbose = 10, 0\n",
    "DEBUG_DOWNSCALE, verbose = 10, 100\n",
    "DEBUG_DOWNSCALE, verbose = 1, 0\n",
    "N_scan = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for eta in np.logspace(-3, -1, N_scan, base=10):\n",
    "    shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, \n",
    "              learning_algorithm='omp', eta=eta, verbose=verbose)\n",
    "    dico = shl.learn_dico()\n",
    "    _ = shl.show_dico(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for transform_n_nonzero_coefs in np.logspace(3, 5, N_scan, base=2):\n",
    "    shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, \n",
    "              learning_algorithm='omp', transform_n_nonzero_coefs=transform_n_nonzero_coefs, verbose=verbose)\n",
    "    dico = shl.learn_dico()\n",
    "    _ = shl.show_dico(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without homeostasis\n",
    "\n",
    "Here,we only ensure the norm ofthe filters is constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for eta in np.logspace(-3, -1.5, N_scan, base=10):\n",
    "    shl = SHL(DEBUG_DOWNSCALE=DEBUG_DOWNSCALE, eta_homeo=0,\n",
    "              learning_algorithm='omp', eta=eta, verbose=verbose)\n",
    "    dico = shl.learn_dico()\n",
    "    _ = shl.show_dico(dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%install_ext http://raw.github.com/jrjohansson/version_information/master/version_information.py\n",
    "%load_ext version_information\n",
    "%version_information numpy, shl_scripts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
